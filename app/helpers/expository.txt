The basic purpose of narrative is to entertain, to gain and hold a readers' interest. However narratives can also be written to teach or inform, to change attitudes / social opinions e.g. soap operas and television dramas that are used to raise topical issues. Narratives sequence people/characters in time and place but differ from recounts in that through the sequencing, the stories set up one or more problems, which must eventually find a way to be resolved. The common structure or basic plan of narrative text is known as the "story grammar". Although there are numerous variations of the story grammar, the typical elements are:
A narrator is a personal character or a non-personal voice that the creator (author) of the story develops to deliver information to the audience, particularly about the plot. The narrator may be a voice devised by the author as an anonymous, non-personal, or stand-alone entity; as the author herself/himself as a character; or as some other fictional or non-fictional character appearing and participating within their own story. The narrator is considered participant if he/she is a character within the story, and non-participant if he/she is an implied character or an omniscient or semi-omniscient being or voice that merely relates the story to the audience without being involved in the actual events. Some stories have multiple narrators to illustrate the storylines of various characters at the same, similar, or different times, thus allowing a more complex, non-singular point of view.
Stream of consciousness is a narrative device that attempts to give the written equivalent of the character's thought processes, either in a loose interior monologue (see below), or in connection to his or her actions. Stream-of-consciousness writing is usually regarded as a special form of interior monologue and is characterized by associative leaps in thought and lack of some or all punctuation.[2] Stream of consciousness and interior monologue are distinguished from dramatic monologue and soliloquy, where the speaker is addressing an audience or a third person, which are chiefly used in poetry or drama. In stream of consciousness the speaker's thought processes are more often depicted as overheard in the mind (or addressed to oneself); it is primarily a fictional device.
The term "Stream of Consciousness" was coined by philosopher and psychologist William James in The Principles of Psychology (1890):
A psychologist is a professional who evaluates and studies behavior and mental processes[1] (see also psychology). Typically, psychologists must have completed a university degree in psychology, which is a master's degree in some countries and a doctorate in others. This definition of psychologist is non-exclusive; in most jurisdictions, members of other professions (such as counselors and psychiatrists) can also evaluate, diagnose, treat, and study mental processes.[2] Some psychologists, such as clinical and counseling psychologists, provide mental health care, and some psychologists, such as social or organizational psychologists conduct research and provide consultation services.
A doctorate (from Latin docere, "to teach") or doctor's degree (from Latin doctor, "teacher") or doctoral degree (from the ancient formalism licentia docendi) is an academic degree awarded by universities that is, in most countries, a research degree that qualifies the holder to teach at the university level in the degree's field, or to work in a specific profession. There are a variety of doctoral degrees, with the most common being the Doctor of Philosophy (PhD), which is awarded in many different fields, ranging from the humanities to the scientific disciplines. There are also some doctorates in the US, such as the Juris Doctor (JD), Doctor of Nursing Practice (DNP) and the Doctor of Medicine (MD), which are generally regarded internationally as professional degrees rather than doctorates, as they are not research degrees and no defense of any dissertation or thesis is performed. Many universities also award honorary doctorates to individuals who have been deemed worthy of special recognition, either for scholarly work or for other contributions to the university or to society.
Contemporary science is typically subdivided into the natural sciences, which study the material universe; the social sciences, which study people and societies; and the formal sciences, which study logic and mathematics. The formal sciences are often excluded as they do not depend on empirical observations.[3] Disciplines which use science, like engineering and medicine, may also be considered to be applied sciences.[4]
From classical antiquity through the 19th century, science as a type of knowledge was more closely linked to philosophy than it is now, and in the Western world the term "natural philosophy" once encompassed fields of study that are today associated with science, such as astronomy, medicine, and physics.[5][nb 3] However, in the Middle East, during the Middle Ages foundations for the scientific method were laid by Ibn al-Haytham in his Book of Optics.[6][7][8][9][10] While the classification of the material world by the ancient Indians and Greeks into air, earth, fire and water was more philosophical, medieval Middle Easterns used practical and experimental observation to classify materials.[11]
In the 17th and 18th centuries, scientists increasingly sought to formulate knowledge in terms of physical laws. Over the course of the 19th century, the word "science" became increasingly associated with the scientific method itself as a disciplined way to study the natural world. It was during this time that scientific disciplines such as biology, chemistry, and physics reached their modern shapes. That same time period also included the origin of the terms "scientist" and "scientific community," the founding of scientific institutions, and the increasing significance of their interactions with society and other aspects of culture.[12][13]
Social science is a major category of academic disciplines, concerned with society and the relationships among individuals within a society. It in turn has many branches, each of which is considered a "social science". The main social sciences include economics, political science, human geography, demography and sociology. In a wider sense, social science also includes some fields in the humanities[1] such as anthropology, archaeology, psychology, jurisprudence, history, and linguistics. The term is also sometimes used to refer specifically to the field of sociology, the original 'science of society', established in the 19th century.
Engineering is the application of mathematics and scientific, economic, social, and practical knowledge in order to invent, innovate, design, build, maintain, research, and improve structures, machines, tools, systems, components, materials, processes and organizations.
The discipline of engineering is extremely broad, and encompasses a range of more specialized fields of engineering, each with a more specific emphasis on particular areas of applied science, technology and types of application.
An explanation is a set of statements constructed to describe a set of facts which clarifies the causes, context, and consequences of those facts. This description may establish rules or laws, and may clarify the existing ones in relation to any objects, or phenomena examined. The components of an explanation can be implicit, and be interwoven with one another.
In scientific research, explanation is one of several purposes for empirical research.[1][2] Explanation is a way to uncover new knowledge, and to report relationships among different aspects of studied phenomena. Explanation attempts to answer the "why" and "how" questions. Explanations have varied explanatory power. The formal hypothesis is the theoretical tool used to verify explanation in empirical research.[3][4]
While arguments attempt to show that something is, will be, or should be the case, explanations try to show why or how something is or will be. If Fred and Joe address the issue of whether or not Fred's cat has fleas, Joe may state: "Fred, your cat has fleas. Observe the cat is scratching right now." Joe has made an argument that the cat has fleas. However, if Fred and Joe agree on the fact that the cat has fleas, they may further question why this is so and put forth an explanation: "The reason the cat has fleas is that the weather has been damp." The difference is that the attempt is not to settle whether or not some claim is true, but to show why it is true.
Justification is the reason why someone properly holds a belief, the explanation as to why the belief is a true one, or an account of how one knows what one knows. In much the same way arguments and explanations may be confused with each other, so too may explanations and justifications. Statements which are justifications of some action take the form of arguments. For example, attempts to justify a theft usually explain the motives (e.g., to feed a starving family).
It is important to be aware when an explanation is not a justification. A criminal profiler may offer an explanation of a suspect's behavior (e.g.; the person lost their job, the person got evicted, etc.). Such statements may help us understand why the person committed the crime, however an uncritical listener may believe the speaker is trying to gain sympathy for the person and his or her actions. It does not follow that a person proposing an explanation has any sympathy for the views or actions being explained. This is an important distinction because we need to be able to understand and explain terrible events and behavior in attempting to discourage it.
A prediction (Latin prï¿½-, "before," and dicere, "to say"), or forecast, is a statement about an uncertain event. It is often, but not always, based upon experience or knowledge. There is no universal agreement about the exact difference between the two terms; different authors and disciplines ascribe different connotations.
In statistics, prediction is a part of statistical inference. One particular approach to such inference is known as predictive inference, but the prediction can be undertaken within any of the several approaches to statistical inference. Indeed, one possible description of statistics is that it provides a means of transferring knowledge about a sample of a population to the whole population, and to other related populations, which is not necessarily the same as prediction over time. When information is transferred across time, often to specific points in time, the process is known as forecasting.[3] Forecasting usually requires time series methods, while prediction is often performed on cross-sectional data.
Mathematical equations and models, and computer models, are frequently used to describe the past and future behaviour of a process within the boundaries of that model. In some cases the probability of an outcome, rather than a specific outcome, can be predicted, for example in much of quantum physics.
Neural Networks (also referred to as connectionist systems) are a computational approach which is based on a large collection of neural units loosely modeling the way the brain solves problems with large clusters of biological neurons connected by axons. Each neural unit is connected with many others, and links can be enforcing or inhibitory in their effect on the activation state of connected neural units. Each individual neural unit may have a summation function which combines the values of all its inputs together. There may be a threshold function or limiting function on each connection and on the unit itself such that it must surpass it before it can propagate to other neurons. These systems are self-learning and trained rather than explicitly programmed and excel in areas where the solution or feature detection is difficult to express in a traditional computer program.
Neural networks typically consist of multiple layers or a cube design, and the signal path traverses from front to back. Back propagation is where the forward stimulation is used to reset weights on the "front" neural units and this is sometimes done in combination with training where the correct result is known. More modern networks are a bit more free flowing in terms of stimulation and inhibition with connections interacting in a much more chaotic and complex fashion. Dynamic neural networks are the most advanced in that they dynamically can, based on rules, form new connections and even new neural units while disabling others.
The goal of the neural network is to solve problems in the same way that the human brain would, although several neural networks are much more abstract. Modern neural network projects typically work with a few thousand to a few million neural units and millions of connections, which is still several orders of magnitude less complex than the human brain and closer to the computing power of a worm.
New brain research often stimulates new patterns in neural networks. One new approach is using connections which span much further and link processing layers rather than always being localized to adjacent neurons. Other research being explored with the different types of signal over time that axons propagate which is more complex than simply on or off.
Artificial intelligence (AI) is intelligence exhibited by machines. In computer science, an ideal "intelligent" machine is a flexible rational agent that perceives its environment and takes actions that maximize its chance of success at some goal.[1] Colloquially, the term "artificial intelligence" is applied when a machine mimics "cognitive" functions that humans associate with other human minds, such as "learning" and "problem solving".[2] As machines become increasingly capable, mental facilities once thought to require intelligence are removed from the definition. For example, optical character recognition is no longer perceived as an exemplar of "artificial intelligence", having become a routine technology.[3] Capabilities currently classified as AI include successfully understanding human speech,[4] competing at a high level in strategic game systems (such as Chess and Go[5]), self-driving cars, and interpreting complex data. Some people also consider AI a danger to humanity if it progresses unabatedly.[6] AI research is divided into subfields[7] that focus on specific problems or on specific approaches or on the use of a particular tool or towards satisfying particular applications.
The central problems (or goals) of AI research include reasoning, knowledge, planning, learning, natural language processing (communication), perception and the ability to move and manipulate objects.[8] General intelligence is among the field's long-term goals.[9] Approaches include statistical methods, computational intelligence, soft computing (e.g. machine learning), and traditional symbolic AI. Many tools are used in AI, including versions of search and mathematical optimization, logic, methods based on probability and economics. The AI field draws upon computer science, mathematics, psychology, linguistics, philosophy, neuroscience and artificial psychology.
The field was founded on the claim that human intelligence "can be so precisely described that a machine can be made to simulate it."[10] This raises philosophical arguments about the nature of the mind and the ethics of creating artificial beings endowed with human-like intelligence, issues which have been explored by myth, fiction and philosophy since antiquity.[11] Attempts to create artificial intelligence have experienced many setbacks, including the ALPAC report of 1966, the abandonment of perceptrons in 1970, the Lighthill Report of 1973, the second AI winter 1987ï¿½1993 and the collapse of the Lisp machine market in 1987. In the twenty-first century AI techniques became an essential part of the technology industry, helping to solve many challenging problems in computer science.[12]
Existential risk from artificial general intelligence is the hypothetical threat that dramatic progress in artificial intelligence (AI) could someday result in human extinction (or some other unrecoverable global catastrophe).[1][2][3] The human race currently dominates other species because the human brain has some distinctive capabilities that the brains of other animals lack. If AI surpasses humanity in general intelligence and becomes "superintelligent", then this new superintelligence could become powerful and difficult to control. Just as the fate of the mountain gorilla depends on human goodwill, so might the fate of humanity depend on the actions of a future machine superintelligence.[4]
The severity of different AI risk scenarios is widely debated, and rests on a number of unresolved questions about future progress in computer science.[5] Two sources of concern are that a sudden and unexpected "intelligence explosion" might take an unprepared human race by surprise, and that controlling a superintelligent machine (or even instilling it with human-compatible values) may be an even harder problem than naively supposed.[1][6]
Machine ethics (or machine morality, computational morality, or computational ethics) is a part of the ethics of artificial intelligence concerned with the moral behavior of artificially intelligent beings.[1] Machine ethics contrasts with roboethics, which is concerned with the moral behavior of humans as they design, construct, use and treat such beings. Machine ethics should not be confused with computer ethics, which focuses on professional behavior towards computers and information.
Before the 21st century the ethics of machines had largely been the subject of science fiction literature, mainly due to computing and artificial intelligence (AI) limitations. These limitations are being overcome through advances in theory and hardware resulting in a renewed focus on the field of artificial intelligence making Machine Ethics a bona fide field of research. The first use of the term seems to be by Mitchell Waldrop in the 1987 AI Magazine article "A Question of Responsibility":
The Universe is all of time and space and its contents.[9][10][11][12] It includes planets, moons, minor planets, stars, galaxies, the contents of intergalactic space, and all matter and energy. The observable universe is about 28 billion parsecs (91 billion light-years) in diameter.[3] The size of the entire Universe is unknown, but there are many hypotheses about the composition and evolution of the Universe.[13]
The earliest scientific models of the Universe were developed by ancient Greek and Indian philosophers and were geocentric, placing the Earth at the center of the Universe.[14][15] Over the centuries, more precise astronomical observations led Nicolaus Copernicus (1473ï¿½1543) to develop the heliocentric model with the Sun at the center of the Solar System. In developing the law of universal gravitation, Sir Isaac Newton (NS: 1643ï¿½1727) built upon Copernicus's work as well as observations by Tycho Brahe (1546ï¿½1601) and Johannes Kepler's (1571ï¿½1630) laws of planetary motion. Further observational improvements led to the realization that our Solar System is located in the Milky Way galaxy and is one of many solar systems and galaxies. It is assumed that galaxies are distributed uniformly and the same in all directions, meaning that the Universe has neither an edge nor a center. Discoveries in the early 20th century have suggested that the Universe had a beginning and that it is expanding[16] at an increasing rate.[17] The majority of mass in the Universe appears to exist in an unknown form called dark matter.
Candy, also called sweets or lollies, is a confection that features sugar as a principal ingredient. The category, called sugar confectionery, encompasses any sweet confection, including chocolate, chewing gum, and sugar candy. Vegetables, fruit, or nuts which have been glazed and coated with sugar are said to be candied.
Physically, candy is characterized by the use of a significant amount of sugar or sugar substitutes. Unlike a cake or loaf of bread that would be shared among many people, candies are usually made in smaller pieces. However, the definition of candy also depends upon how people treat the food. Unlike sweet pastries served for a dessert course at the end of a meal, candies are normally eaten casually, often with the fingers, as a snack between meals. Each culture has its own ideas of what constitutes candy rather than dessert. The same food may be a candy in one culture and a dessert in another.[1]
Between the 6th and 4th centuries BCE, the Persians, followed by the Greeks, discovered the people in India and their "reeds that produce honey without bees". They adopted and then spread sugar and sugarcane agriculture.[5] Sugarcane is indigenous to tropical South and Southeast Asia, while the word sugar is derived from the Sanskrit word Sharkara.[6] Pieces of sugar were produced by boiling sugarcane juice in ancient India and consumed as Khanda, dubbed as the original candy.[7][8][9][10]
Before sugar was readily available, candy was based on honey.[11] Honey was used in Ancient China, Middle East, Egypt, Greece and the Roman Empire to coat fruits and flowers to preserve them or to create forms of candy.[12] Candy is still served in this form today, though now it is more typically seen as a type of garnish.
Before the Industrial Revolution, candy was often considered a form of medicine, either used to calm the digestive system or cool a sore throat. In the Middle Ages candy appeared on the tables of only the most wealthy at first. At that time, it began as a combination of spices and sugar that was used as an aid to digestive problems. Digestive problems were very common during this time due to the constant consumption of food that was neither fresh nor well balanced. Banquet hosts would typically serve these types of 'candies' at banquets for their guests. One of these candies, sometimes called chamber spice, was made with cloves, ginger, aniseed, juniper berries, almonds and pine kernels dipped in melted sugar.[12]
The candy business underwent a drastic change in the 1830s when technological advances and the availability of sugar opened up the market. The new market was not only for the enjoyment of the rich but also for the pleasure of the working class. There was also an increasing market for children. While some fine confectioners remained, the candy store became a staple of the child of the American working class. Penny candies epitomized this transformation of candy. Penny candy became the first material good that children spent their own money on. For this reason, candy store-owners relied almost entirely on the business of children to keep them running. Even penny candies were directly descended from medicated lozenges that held bitter medicine in a hard sugar coating.[16]
In 1847, the invention of the candy press (also known as a toy machine) made it possible to produce multiple shapes and sizes of candy at once. In 1851, confectioners began to use a revolving steam pan to assist in boiling sugar. This transformation meant that the candy maker was no longer required to continuously stir the boiling sugar. The heat from the surface of the pan was also much more evenly distributed and made it less likely the sugar would burn. These innovations made it possible for only one or two people to successfully run a candy business.[15]
Candy is made by dissolving sugar in water or milk to form a syrup, which is boiled until it reaches the desired concentration or starts to caramelize. Candy comes in a wide variety of textures, from soft and chewy to hard and brittle. The texture of candy depends on the ingredients and the temperatures that the candy is processed at.
The final texture of sugar candy depends primarily on the sugar concentration. As the syrup is heated, it boils, water evaporates, the sugar concentration increases and the boiling point rises. A given temperature corresponds to a particular sugar concentration. These are called sugar stages. In general, higher temperatures and greater sugar concentrations result in hard, brittle candies, and lower temperatures result in softer candies.[17] Once the syrup reaches 171 ï¿½C (340 ï¿½F) or higher, the sucrose molecules break down into many simpler sugars, creating an amber-colored substance known as caramel. This should not be confused with caramel candy, although it is the candy's main flavoring.
Most candies are made commercially. The industry relies significantly on trade secret protection, because candy recipes cannot be copyrighted or patented effectively, but are very difficult to duplicate exactly. Seemingly minor differences in the machinery, temperature, or timing of the candy-making process can cause noticeable differences in the final product.[18]
Knowledge is a familiarity, awareness or understanding of someone or something, such as facts, information, descriptions, or skills, which is acquired through experience or education by perceiving, discovering, or learning.
Knowledge can refer to a theoretical or practical understanding of a subject. It can be implicit (as with practical skill or expertise) or explicit (as with the theoretical understanding of a subject); it can be more or less formal or systematic.[1] In philosophy, the study of knowledge is called epistemology; the philosopher Plato famously defined knowledge as "justified true belief", though this definition is now agreed by most analytic philosophers to be problematic because of the Gettier problems. However, several definitions of knowledge and theories to explain it exist.
The definition of knowledge is a matter of ongoing debate among philosophers in the field of epistemology. The classical definition, described but not ultimately endorsed by Plato,[4] specifies that a statement must meet three criteria in order to be considered knowledge: it must be justified, true, and believed. Some claim that these conditions are not sufficient, as Gettier case examples allegedly demonstrate. There are a number of alternatives proposed, including Robert Nozick's arguments for a requirement that knowledge 'tracks the truth' and Simon Blackburn's additional requirement that we do not want to say that those who meet any of these conditions 'through a defect, flaw, or failure' have knowledge. Richard Kirkham suggests that our definition of knowledge requires that the evidence for the belief necessitates its truth.[5]
Symbolic representations can be used to indicate meaning and can be thought of as a dynamic process. Hence the transfer of the symbolic representation can be viewed as one ascription process whereby knowledge can be transferred. Other forms of communication include observation and imitation, verbal exchange, and audio and video recordings. Philosophers of language and semioticians construct and analyze theories of knowledge transfer or communication.
While many would agree that one of the most universal and significant tools for the transfer of knowledge is writing and reading (of many kinds), argument over the usefulness of the written word exists nonetheless, with some scholars skeptical of its impact on societies. In his collection of essays Technopoly, Neil Postman demonstrates the argument against the use of writing through an excerpt from Plato's work Phaedrus (Postman, Neil (1992) Technopoly, Vintage, New York, pp 73). In this excerpt, the scholar Socrates recounts the story of Thamus, the Egyptian king and Theuth the inventor of the written word. In this story, Theuth presents his new invention "writing" to King Thamus, telling Thamus that his new invention "will improve both the wisdom and memory of the Egyptians" (Postman, Neil (1992) Technopoly, Vintage, New York, pp 74). King Thamus is skeptical of this new invention and rejects it as a tool of recollection rather than retained knowledge. He argues that the written word will infect the Egyptian people with fake knowledge as they will be able to attain facts and stories from an external source and will no longer be forced to mentally retain large quantities of knowledge themselves (Postman, Neil (1992) Technopoly, Vintage, New York,pp 74).
Classical early modern theories of knowledge, especially those advancing the influential empiricism of the philosopher John Locke, were based implicitly or explicitly on a model of the mind which likened ideas to words.[8] This analogy between language and thought laid the foundation for a graphic conception of knowledge in which the mind was treated as a table, a container of content, that had to be stocked with facts reduced to letters, numbers or symbols. This created a situation in which the spatial alignment of words on the page carried great cognitive weight, so much so that educators paid very close attention to the visual structure of information on the page and in notebooks.[9]
Media theorists like Andrew Robinson emphasise that the visual depiction of knowledge in the modern world was often seen as being 'truer' than oral knowledge. This plays into a longstanding analytic notion in the Western intellectual tradition in which verbal communication is generally thought to lend itself to the spread of falsehoods as much as written communication. It is harder to preserve records of what was said or who originally said it ï¿½ usually neither the source nor the content can be verified. Gossip and rumors are examples prevalent in both media. As to the value of writing, the extent of human knowledge is now so great, and the people interested in a piece of knowledge so separated in time and space, that writing is considered central to capturing and sharing it.
Basketball is a sport, generally played by two teams of five players on a rectangular court. The objective is to shoot a ball through a hoop 18 inches (46 cm) in diameter and mounted at a height of 10 feet (3.048 m) to backboards at each end of the court.
A team can score a field goal by shooting the ball through the basket being defended by the opposition team during regular play. A field goal scores three points for the shooting team if the player shoots from behind the three-point line, and two points if shot from in front of the line. A team can also score via free throws, which are worth one point, after the other team is assessed with certain fouls. The team with the most points at the end of the game wins, but additional time (overtime) is issued when the score is tied at the end of regulation. The ball can be advanced on the court by throwing it to a teammate, or by bouncing it while walking or running (dribbling). It is a violation to lift, or drag, one's pivot foot without dribbling the ball, to carry it, or to hold the ball with both hands then resume dribbling.
There are many techniques for ball-handlingï¿½shooting, passing, dribbling, and rebounding. Basketball teams generally have player positions, the tallest and strongest members of a team are called a center or power forward, while slightly shorter and more agile players are called small forward, and the shortest players or those who possess the best ball handling skills are called a point guard or shooting guard. The point guard directs the on court action of the team, implementing the coach's game plan, and managing the execution of offensive and defensive plays (player positioning).
Basketball is one of the world's most popular and widely viewed sports.[1] The National Basketball Association (NBA) is the most popular and widely considered to be the highest level of professional basketball in the world and NBA players are the world's best paid sportsmen, by average annual salary per player.[2][3] Outside North America, the top clubs from national leagues qualify to continental championships such as the Euroleague and FIBA Americas League. The FIBA Basketball World Cup attracts the top national teams from around the world. Each continent hosts regional competitions for national teams, like EuroBasket and FIBA Americas Championship.
The FIBA Women's Basketball World Cup features the top national women's basketball teams from continental championships. The main North American league is the WNBA, whereas the EuroLeague Women has been dominated by teams from the Russian Women's Basketball Premier League. In women's basketball the ball used is smaller than that used in men's basketball. The circumference of the ball for women is 28 1/2 to 29 inches. The basketball in the men's game is between 29 1/2 and 30 inches.
Harry Potter is a series of fantasy novels written by British author J. K. Rowling. The novels chronicle the life of a young wizard, Harry Potter, and his friends Hermione Granger and Ron Weasley, all of whom are students at Hogwarts School of Witchcraft and Wizardry. The main story arc concerns Harry's struggle against Lord Voldemort, a dark wizard who intends to become immortal, overthrow the wizard governing body known as the Ministry of Magic, and subjugate all wizards and Muggles.
Since the release of the first novel, Harry Potter and the Philosopher's Stone, on 26 June 1997, the books have found immense popularity, critical acclaim and commercial success worldwide. They have attracted a wide adult audience as well as younger readers, and are often considered cornerstones of modern young adult literature.[3] The series has also had its share of criticism, including concern about the increasingly dark tone as the series progressed, as well as the often gruesome and graphic violence it depicts. As of July 2013, the books have sold more than 450 million copies worldwide, making them the best-selling book series in history, and have been translated into seventy-three languages.[4][5] The last four books consecutively set records as the fastest-selling books in history, with the final instalment selling roughly eleven million copies in the United States within twenty-four hours of its release.
The series was originally published in English by two major publishers, Bloomsbury in the United Kingdom and Scholastic Press in the United States. A play, Harry Potter and the Cursed Child, based on a story by Rowling, premiered in London on 30 July 2016 at the Palace Theatre, and its script was published by Little, Brown as the eighth book in the series.[6] The original seven books were adapted into an eight-part film series by Warner Bros. Pictures, which has become the second highest-grossing film series of all time as of August 2015. The franchise has also generated much tie-in merchandise, making the Harry Potter brand worth in excess of $25 billion.[7]
A series of many genres, including fantasy, drama, coming of age and the British school story (which includes elements of mystery, thriller, adventure, horror and romance), the world of Harry Potter explores numerous themes and includes many cultural meanings and references.[8] According to Rowling, the main theme is death.[9] Other major themes in the series include prejudice, corruption, and madness.[10]
The success of the books and films has ensured that the Harry Potter franchise continues to expand, with numerous derivative works, a travelling exhibition that premiered in Chicago in 2009, a studio tour in London that opened in 2012, a digital platform on which J.K. Rowling updates the series with new information and insight, and a pentalogy of spin-off films premiering in November 2016, among many other developments. Most recently, themed attractions, collectively known as The Wizarding World of Harry Potter, have been built at several Universal Parks & Resorts amusement parks around the world.
The central character in the series is Harry Potter, an English orphan who discovers, at the age of eleven, that he is a wizard, though he lives in the ordinary world of non-magical people known as Muggles.[11] The wizarding world exists parallel to the Muggle world, albeit hidden and in secrecy. His magical ability is inborn and children with such abilities are invited to attend exclusive magic schools that teach the necessary skills to succeed in the wizarding world.[12] Harry becomes a student at Hogwarts School of Witchcraft and Wizardry, a wizarding academy in Scotland and it is here where most of the events in the series take place. As Harry develops through his adolescence, he learns to overcome the problems that face him: magical, social and emotional, including ordinary teenage challenges such as friendships, infatuation, romantic relationships, schoolwork and exams, anxiety, depression, stress, and the greater test of preparing himself for the confrontation, that lies ahead, in wizarding Britain's increasingly-violent second wizarding war.[13]
With Hagrid's help, Harry prepares for and undertakes his first year of study at Hogwarts. As Harry begins to explore the magical world, the reader is introduced to many of the primary locations used throughout the series. Harry meets most of the main characters and gains his two closest friends: Ron Weasley, a fun-loving member of an ancient, large, happy, but poor wizarding family, and Hermione Granger, a gifted and very hardworking witch of non-magical parentage.[17][18] Harry also encounters the school's potions master, Severus Snape, who displays a conspicuously deep and abiding dislike for him, and the Defence Against the Dark Arts teacher, Quirinus Quirrell, who later turns out to be allied with Lord Voldemort. The first book concludes with Harry's second confrontation with Lord Voldemort, who, in his quest for immortality, yearns to gain the power of the Philosopher's Stone, a substance that bestows everlasting life.[17]
The Kingdom of Stormwind[4] (also called Kingdom of Azeroth[5]) is a human kingdom,[6][7] nation[8][9][10] and country[11] on the southern continent of the Eastern Kingdoms, Azeroth - with its capital at Stormwind City.[3] Originally settled by descendants of the Arathi,[12] Stormwind flourished until the First War when it was conquered and laid waste by the Orcish Horde. The survivors of that conflict, led by Anduin Lothar, sought refuge in Lordaeron, but were able to reclaim their homeland at the end of the Second War with the aid of the Alliance of Lordaeron.[13]
Stormwind is ruled by the House of Wrynn from their court at Stormwind Keep. Presently, the throne is occupied by King Anduin Wrynn - son of Varian Wrynn, who sacrificed himself to save many Alliance members during the Assault of the Broken Shore. Also aiding in the governing of Stormwind is the House of Nobles, an executive branch of Stormwind's government who currently aids King Varian in Stormwind, during the difficult times the Alliance face now. 
Stormwind's control and influence now reaches across all of the continent of Azeroth but it is unknown if Stranglethorn Vale falls under its jurisdiction due to the multitudes of other factions each having some sort of claim to the land. 
The Koprulu sectorï¿½nicknamed the Terran Sectorï¿½ is a sector in space colonized by terrans in the shadow of protoss space. It is situated on the galactic fringe of the Milky Way, sixty thousand light years from Earth.[1] During the Guild Wars, the sector was divided into at least five zones.[2] Most of its planets are inhospitable.[3] Still, the sector offers a vast range of foodstuffs.[4]
With the arrival of humanity in the sector, it was noted that the sector had been inhabited, or at least graced by alien species, given the prevalence of lifeforms that appeared to have been formed artificially. But by the time of terran settlement, while the Koprulu sector featured an extensive range of flora and fauna, no sapient species inhabited it.[7]
Tyria, also known as Central Tyria, is a continent in the world of the same name. The geography of the continent ranges from the dense jungles of the Maguuma, to high mountains of the Shiverpeaks, to forests of the Blood Legion Homelands and Ascalon, to the sandy Crystal Desert. Beneath the land stretch the interconnected caverns of the Depths of Tyria which hold ancient structures of unknown origins that date back to a time before humanity. Over ten thousand years ago, the Great Giants once roamed the land. Before all recorded history, the Elder Dragons roamed - whether before or during the time of the Giganticus Lupicus is still unknown. And over a thousand years ago, before the exodus, the human gods lived in the continent of Tyria. Throughout the ages, the continent has changed - from seas turning to deserts, to sinking kingdoms and scorching lands, to flooding cities, and to deserts turning green by diverted rivers - and may continue to change. 
Westeros is one of the four known continents in the known world, the others being Essos, Sothoryos, and Ulthos. Most of the area of Westeros is covered by a political entity known as the Seven Kingdoms, while the far north beyond the Wall includes the free folk. The closest foreign nations to Westeros are the Free Cities, a collection of independent city-states across the narrow sea in western Essos. To the south of Westeros lie the Summer Isles.
